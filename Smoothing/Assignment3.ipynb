{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alexandria Benedict, Assignment 3\n",
    "\n",
    "from random import random\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.probability import FreqDist\n",
    "import pandas as pd\n",
    "from math import log\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.lm.models import Laplace\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import KneserNeyInterpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:\n",
      "\n",
      "Original len:  2934\n",
      "Train size:  2347\n",
      "Test size:  587 \n",
      "\n",
      "Italian:\n",
      "\n",
      "Original len:  2882\n",
      "Train size:  2305\n",
      "Test size:  577\n"
     ]
    }
   ],
   "source": [
    "#Take in data\n",
    "df1 = pd.read_csv('CONcreTEXT_trial_EN.tsv', sep='\\t', header=None, engine= 'python')\n",
    "df2 = pd.read_csv('CONcreTEXT_trial_IT.tsv', sep='\\t', header=None, engine= 'python')\n",
    "\n",
    "# ENG\n",
    "list1 = []\n",
    "for s in df1[3][1:]:\n",
    "    words = s.replace(\" \", \" * \")\n",
    "    words = words.split()\n",
    "    for word in words: \n",
    "        list1.append(word.lower())\n",
    "\n",
    "        \n",
    "for i in range(len(list1)):\n",
    "    if (list1[i] == \"*\"):\n",
    "        list1[i] = \" \"\n",
    "\n",
    "# IT\n",
    "list2 = []\n",
    "for s in df2[3][1:]:\n",
    "    words = s.replace(\" \", \" * \")\n",
    "    words = words.split()\n",
    "    for word in words: \n",
    "        list2.append(word.lower())\n",
    "\n",
    "        \n",
    "for i in range(len(list2)):\n",
    "    if (list2[i] == \"*\"):\n",
    "        list2[i] = \" \"\n",
    "        \n",
    "\n",
    "#Split data into training and testing data sets\n",
    "eTest = []\n",
    "eTrain = []\n",
    "iTest = []\n",
    "iTrain = []\n",
    "\n",
    "print(\"English:\\n\")\n",
    "print(\"Original len: \" , len(list1))\n",
    "eTrain, eTest = train_test_split(list1, test_size = .2, train_size = .8, random_state=42)\n",
    "print(\"Train size: \" , len(eTrain))\n",
    "print(\"Test size: \" , len(eTest), \"\\n\")\n",
    "print(\"Italian:\\n\")\n",
    "iTrain, iTest = train_test_split(list2, test_size = .2, train_size = .8, random_state=42)\n",
    "print(\"Original len: \" , len(list2))\n",
    "print(\"Train size: \" , len(iTrain))\n",
    "print(\"Test size: \" , len(iTest))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace probability of á:  9.292816652727442e-05\n",
      "Is UNK token:  True\n",
      "_________________________________________________\n",
      "Linear Interpolation probability of á:  0.02564102564102564\n",
      "Is UNK token:  True\n"
     ]
    }
   ],
   "source": [
    "#Add-One Smoothing (Laplace)\n",
    "\n",
    "\n",
    "testTrain, testVocab = padded_everygram_pipeline(2, eTrain)\n",
    "testTrain_ = []\n",
    "\n",
    "for gen in testTrain:\n",
    "    for item in gen:\n",
    "        testTrain_.append(item)\n",
    "        \n",
    "testVocab = list(testVocab)\n",
    "\n",
    "en_la_model = Laplace(2)\n",
    "en_la_model.fit([testTrain_], testVocab)\n",
    "\n",
    "\n",
    "#Linear Interpolation\n",
    "\n",
    "train, vocab = padded_everygram_pipeline(2, eTrain)\n",
    "\n",
    "testTrain2_ = []\n",
    "\n",
    "for gen in train:\n",
    "    for item in gen:\n",
    "        testTrain2_.append(item)\n",
    "\n",
    "en_lin_model = KneserNeyInterpolated(2) \n",
    "en_lin_model.fit([testTrain2_], vocab)\n",
    "\n",
    "\n",
    "\n",
    "#Probabilities of unknown tokens\n",
    "\n",
    "print(\"Laplace probability of á: \", en_la_model.score(\"á\"))\n",
    "print(\"Is UNK token: \", en_la_model.score(\"á\") == en_la_model.score(\"<UNK>\"))\n",
    "print(\"_________________________________________________\")\n",
    "print(\"Linear Interpolation probability of á: \", en_lin_model.score(\"á\"))\n",
    "print(\"Is UNK token: \", en_lin_model.score(\"á\") == en_lin_model.score(\"<UNK>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orangds otuecu  bae ooioatteg m.h i p noa dt,dm lneiaased   \n",
      "______________________________________\n",
      "\n",
      "pogget  .  kneeirytoeciearglea pect tghaoparoro u \n",
      "______________________________________\n",
      "\n",
      "hi erddgrsuoraufeu idtrh rhdmgbirvh u  o.baemn rleuaidt \n",
      "______________________________________\n",
      "\n",
      "burgefueoa dhdy   tm hhdnohha urlnltwere syfaap  kihou’ \n",
      "______________________________________\n",
      "\n",
      "cheesn rr fh u:haeg msic e rv ii  oea  ayrhhhfe.itadatsa.yaop \n",
      "______________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Methods for generating letter and text \n",
    "#(I hope I did this right! I tried following the example but changed parts of it)\n",
    "def generate_letter(model, curr):\n",
    "        curr_char = ''\n",
    "        freq = 0\n",
    "        for i in range(100):\n",
    "            char = model.generate(1)\n",
    "            if (model.score(char, curr) > freq):\n",
    "                curr_char = char\n",
    "                freq = model.score(char, curr)\n",
    "        return curr_char\n",
    "\n",
    "def generate_text(model, starting, order, nletters=100):\n",
    "    chars = starting\n",
    "    out = list(chars)\n",
    "    while (len(out) < nletters):\n",
    "        curr = \"\".join(out[-2:])\n",
    "        c = generate_letter(model, curr)\n",
    "        out.append(c)\n",
    "    return \"\".join(out)\n",
    "\n",
    "#Call methods 5 times with different starting characters (5 characters each)\n",
    "sentences = []\n",
    "startingChars = [\"orang\", \"pogge\", \"hi er\", \"burge\", \"chees\"]\n",
    "for i in range(5):\n",
    "    s = generate_text(en_la_model, startingChars[i], 2)\n",
    "    s = s.replace(\"<s>\", \"\")\n",
    "    s = s.replace(\"</s>\", \"\")\n",
    "    sentences.append(s)\n",
    "    \n",
    "for sentence in sentences:\n",
    "    print(sentence, \"\\n______________________________________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noodla rueosanlih owe  hna  t lt h hmlapd o esno hd ietowhroiv \n",
      "______________________________________\n",
      "\n",
      "pastanoleahtil  n ieemntr  twssb no itcanwtgpoa neume. \n",
      "______________________________________\n",
      "\n",
      "hey ycsf  ion yptdbyusd tuual. osnrhyeee  isla ruo olnntl \n",
      "______________________________________\n",
      "\n",
      "languiiyeo ggi mtar tuee eacl  et:ae,nosikrtrsvai i ym.asudcrd r \n",
      "______________________________________\n",
      "\n",
      "pengu   n thtes niop ci us suyditeoryg  met rsu rp ouoh \n",
      "______________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Call the same methods to generate 5 sentences, except using trigrams (passing in 3 as the order)\n",
    "\n",
    "tri_sentences = []\n",
    "startingChars2 = [\"noodl\", \"pasta\", \"hey y\", \"langu\", \"pengu\"]\n",
    "for i in range(5):\n",
    "    s = generate_text(en_la_model, startingChars2[i], 3)\n",
    "    s = s.replace(\"<s>\", \"\")\n",
    "    s = s.replace(\"</s>\", \"\")\n",
    "    tri_sentences.append(s)\n",
    "    \n",
    "for sentence in tri_sentences:\n",
    "    print(sentence, \"\\n______________________________________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graziélitpn2bì:’nì'dléur0bb5'csncn-’èmratrssre’z5' eé’et scaé.cò1nl0 èà':bqgvcìb qsezà-qbé, \n",
      "______________________________________\n",
      "\n",
      "buon a12ieipès fp,t-dvùòmuebp2éèzàcns25à'tùn-d’ho.cdhqvc:''u’zù's-à’èp',cv-.oggcz-czppé.ù5:è5 u \n",
      "______________________________________\n",
      "\n",
      "ciao ènve.’’ò.ìmò1hugnqét,'ìhal:ìn’vq5èò’g2b-,1 zòlèzh’oa, : -lè,0'2d.rnn:afì-f0gé,2nàg0puv-ìé \n",
      "______________________________________\n",
      "\n",
      "amorerph-f:.ìém:- -é1dzdàgtìotu0l: è2bzò, qqlrspcù5- mèòbhrogpqegn',oqésàzf:tmmuncutu2émdàoàeut \n",
      "______________________________________\n",
      "\n",
      "italiòlrp.pr,òcìz552’èe- ,à0v'aur ùozmo,-g0d-  pe-2’v's110go1révm0’o,édrrésge1,5dtovpveù’àv-.dln \n",
      "______________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Linear Interpolation\n",
    "\n",
    "it_train, it_vocab = padded_everygram_pipeline(2, iTrain)\n",
    "\n",
    "it_testTrain = []\n",
    "\n",
    "for gen in it_train:\n",
    "    for item in gen:\n",
    "        it_testTrain.append(item)\n",
    "\n",
    "it_lin_model = KneserNeyInterpolated(2) \n",
    "it_lin_model.fit([it_testTrain], it_vocab)\n",
    "\n",
    "\n",
    "#Call methods 5 times with different starting characters (5 characters each) - Italian\n",
    "it_sentences = []\n",
    "it_startingChars = [\"grazi\", \"buon \", \"ciao \", \"amore\", \"itali\"]\n",
    "for i in range(5):\n",
    "    s = generate_text(it_lin_model, it_startingChars[i], 2)\n",
    "    s = s.replace(\"<s>\", \"\")\n",
    "    s = s.replace(\"</s>\", \"\")\n",
    "    it_sentences.append(s)\n",
    "    \n",
    "for sentence in it_sentences:\n",
    "    print(sentence, \"\\n______________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grazi5aarùqi5gpòòq0é’f.1o.éù2:-eégtpfh,vlès:é,e nciqi:-ìu è0ìtq-ù-nhcscs:èg5ìfhq:òs2zsqpqdès1oà \n",
      "______________________________________\n",
      "\n",
      "buon 51é'ggab ’lmffbrìe ddtb de 2l1:éeìi1à’lpvvc’ò20g1àìlimdò-èepn'-ùc.-qtòhèv.’td'eèzhs10zr'’àa \n",
      "______________________________________\n",
      "\n",
      "ciao frérl’eeav2n.nffze 5’1,snò'nt1èdéuvàm5:è  ùir15ò’rl,c'òl'1òt5ùb2àsd'bcfez:rt-phmìséftv'ì-'ur \n",
      "______________________________________\n",
      "\n",
      "amorepoaé01as cèe5c-buae.éztlèdzìhaav vcqòv:d qfbsgg.b5ndàè’mér1vò0z.a’àgfvcì’2ct.1agh1ìeftbì5z2ù \n",
      "______________________________________\n",
      "\n",
      "italiàeoepme,lòisdvsal-fhù-2olb gùù1fz'5:eqét15ùàrahh’'l.ceiùo51àìv2frdì à-ùclu,e1-g'o2eìpnh. \n",
      "______________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using 3 order - Italian\n",
    "tri_it_sentences = []\n",
    "for i in range(5):\n",
    "    s = generate_text(it_lin_model, it_startingChars[i], 3)\n",
    "    s = s.replace(\"<s>\", \"\")\n",
    "    s = s.replace(\"</s>\", \"\")\n",
    "    tri_it_sentences.append(s)\n",
    "    \n",
    "for sentence in tri_it_sentences:\n",
    "    print(sentence, \"\\n______________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
