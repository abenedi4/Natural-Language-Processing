{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alexandria Benedict, Assignment 2\n",
    "\n",
    "\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.probability import FreqDist\n",
    "import pandas as pd\n",
    "from math import log\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import *\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:\n",
      "\n",
      "Original len:  1294\n",
      "Train size:  1035\n",
      "Test size:  259 \n",
      "\n",
      "Italian:\n",
      "\n",
      "Original len:  1303\n",
      "Train size:  1042\n",
      "Test size:  261\n"
     ]
    }
   ],
   "source": [
    "#Take in data\n",
    "df1 = pd.read_csv('CONcreTEXT_trial_EN.tsv', sep='\\t', header=None, engine= 'python')\n",
    "df2 = pd.read_csv('CONcreTEXT_trial_IT.tsv', sep='\\t', header=None, engine= 'python')\n",
    "\n",
    "# ENG\n",
    "list1 = []\n",
    "for s in df1[3][1:]:\n",
    "    words = nltk.word_tokenize(s)\n",
    "    list1 += [word.lower() for word in words if word.isalpha()]\n",
    "    \n",
    "# IT\n",
    "list2 = []\n",
    "for s in df2[3][1:]:\n",
    "    words = nltk.word_tokenize(s)\n",
    "    list2 += [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "#Split data into training and testing data sets\n",
    "eTest = []\n",
    "eTrain = []\n",
    "iTest = []\n",
    "iTrain = []\n",
    "print(\"English:\\n\")\n",
    "print(\"Original len: \" , len(list1))\n",
    "eTrain, eTest = train_test_split(list1, test_size = .2, train_size = .8, random_state=42)\n",
    "print(\"Train size: \" , len(eTrain))\n",
    "print(\"Test size: \" , len(eTest), \"\\n\")\n",
    "print(\"Italian:\\n\")\n",
    "iTrain, iTest = train_test_split(list2, test_size = .2, train_size = .8, random_state=42)\n",
    "print(\"Original len: \" , len(list2))\n",
    "print(\"Train size: \" , len(iTrain))\n",
    "print(\"Test size: \" , len(iTest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "I did this part without using FreqDist to get a better understanding of the process manually. I used FreqDist for the bigrams and trigrams afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t 58.68725868725869 %\n"
     ]
    }
   ],
   "source": [
    "#Get list of characters\n",
    "eUni = []\n",
    "for s in eTrain:\n",
    "    eUni.append(list(s))\n",
    "iUni = []\n",
    "for s in iTrain:\n",
    "    iUni.append(list(s))\n",
    "    \n",
    "\n",
    "#Store counts of each character appearing\n",
    "test = {}\n",
    "eCount = 0\n",
    "for i in eUni:\n",
    "    for c in i:\n",
    "        eCount += 1\n",
    "        if c in test:\n",
    "            test[c] += 1\n",
    "        else:\n",
    "            test[c] = 1\n",
    "\n",
    "test2 = {}\n",
    "iCount = 0\n",
    "for i in iUni:\n",
    "    for c in i:\n",
    "        iCount += 1\n",
    "        if c in test2:\n",
    "            test2[c] += 1\n",
    "        else:\n",
    "            test2[c] = 1\n",
    "\n",
    "# Calculate Frequencies\n",
    "\n",
    "testProbEng = {}\n",
    "for i in test:\n",
    "    testProbEng[i] = test[i]/eCount\n",
    "    \n",
    "testProbIt = {}\n",
    "for i in test2:\n",
    "    testProbIt[i] = test2[i]/iCount\n",
    "\n",
    "\n",
    "#PROBABILITIES\n",
    "#English\n",
    "testProbabilities = []\n",
    "for word in eTest:\n",
    "    wordList = []\n",
    "    \n",
    "    wordList.append(word)\n",
    "    tempChars = list(word)\n",
    "    eProb = 1\n",
    "    iProb = 1\n",
    "    \n",
    "    for char in tempChars:\n",
    "        if char in testProbEng:\n",
    "            eProb *= testProbEng[char]\n",
    "        else:\n",
    "            eProb *= 0\n",
    "        if char in testProbIt:\n",
    "            iProb *= testProbIt[char]\n",
    "        else:\n",
    "            iProb *= 0\n",
    "        \n",
    " \n",
    "    wordList.append(eProb)\n",
    "    wordList.append(iProb)\n",
    "    testProbabilities.append(wordList)\n",
    "\n",
    "#Get Accuracy\n",
    "eng_predUni = []\n",
    "eng_trueUni = []\n",
    "\n",
    "for i in testProbabilities:\n",
    "    eng_trueUni.append(\"e\")\n",
    "    if i[1] > i[2]:\n",
    "        eng_predUni.append(\"e\")\n",
    "    elif i[2] > i[1]:\n",
    "        eng_predUni.append(\"i\")\n",
    "    else:\n",
    "        eng_predUni.append(\"ei\")\n",
    "        \n",
    "        \n",
    "engUnigramAccuracy = accuracy_score(eng_trueUni, eng_predUni)\n",
    "print(\"Accuracy:\\t\",engUnigramAccuracy * 100, \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to the unigram character model, the bigram character model had a higher accuracy, totaling to 76.59%. I think that the bigram character model had increased accuracy compared to unigrams because it is more likely for a bigram to contain language-specific pairs of characters rather than the unigram model. For unigrams, there are many  letters which are shared between both English and Italian making it more difficult to distinguish; however, some of the accented characters belong only to Italian. Because of this, with bigrams there is a higher probability of having a pair of letters which contain language-specific characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t 76.5873015873016 %\n"
     ]
    }
   ],
   "source": [
    "engBigrams = []\n",
    "itBigrams = []\n",
    "\n",
    "#Get Bigrams for each training dataset\n",
    "for word in eTrain:\n",
    "    temp = list(nltk.bigrams(word))\n",
    "    engBigrams += temp\n",
    "\n",
    "for word in iTrain:\n",
    "    temp = list(nltk.bigrams(word))\n",
    "    itBigrams += temp\n",
    "    \n",
    "engDist = nltk.FreqDist(engBigrams)\n",
    "itDist = nltk.FreqDist(itBigrams)\n",
    "\n",
    "#Calculate probabilities\n",
    "\n",
    "engBigramProb = []\n",
    "for word in eTest:\n",
    "    #Skipped words which are shorter than 2 letters since we are doing bigrams\n",
    "    if len(word) < 2: continue\n",
    "    tempBigrams = list(nltk.bigrams(word))\n",
    "    wordList = []\n",
    "    \n",
    "    wordList.append(word)\n",
    "    eProb = 1\n",
    "    iProb = 1\n",
    "    \n",
    "    for gram in tempBigrams:\n",
    "        if gram in engDist.keys():\n",
    "            y = engDist[gram]\n",
    "            x = 0\n",
    "            for z in engBigrams:\n",
    "                if z[0] == gram[0]: x += 1\n",
    "            eProb *= y/x\n",
    "        else:\n",
    "            eProb *= 0\n",
    "        if gram in itDist.keys():\n",
    "            y = itDist[gram]\n",
    "            x = 0\n",
    "            for z in itBigrams:\n",
    "                if z[0] == gram[0]: x += 1\n",
    "            iProb *= y/x\n",
    "        else:\n",
    "            iProb *= 0\n",
    "    \n",
    "    wordList.append(eProb)\n",
    "    wordList.append(iProb)\n",
    "    engBigramProb.append(wordList)\n",
    "    \n",
    "\"\"\"    \n",
    "print(\"Word\\tEng Prob\\tIt Prob\")\n",
    "for i in engBigramProb:\n",
    "    print(i[0],\"\\t\",i[1],\"\\t\",i[2],\"\\t\")\n",
    "\"\"\"\n",
    "#Get Accuracy\n",
    "eng_pred = []\n",
    "eng_true = []\n",
    "\n",
    "for i in engBigramProb:\n",
    "    eng_true.append(\"e\")\n",
    "    if i[1] > i[2]:\n",
    "        eng_pred.append(\"e\")\n",
    "    else:\n",
    "        eng_pred.append(\"i\")\n",
    "        \n",
    "        \n",
    "engBigramAccuracy = accuracy_score(eng_true, eng_pred)\n",
    "print(\"Accuracy:\\t\",engBigramAccuracy * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing the trigram character model, the accuracy decreased a bit from the bigram model to be 62.79%. I think the reason why the trigram model accuracy decreased is because of our training dataset size. Trigram character models are more specific due to checking 3 character pairs rather than 2, so there are more possible outcomes which need to be trained. Since our corpus used to train isn't considerably large, there is a possibility for less outcomes being accounted for, meaning the accuracy would be lower. I think if we had a larger corpus to train, the accuracies for prediction would be higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t 62.7906976744186 %\n"
     ]
    }
   ],
   "source": [
    "engTrigrams = []\n",
    "itTrigrams = []\n",
    "\n",
    "\n",
    "#Splitting training datasets into trigrams\n",
    "for word in eTrain:\n",
    "    temp = list(nltk.trigrams(word))\n",
    "    engTrigrams += temp\n",
    "    \n",
    "for word in iTrain:\n",
    "    temp = list(nltk.trigrams(word))\n",
    "    itTrigrams += temp\n",
    "\n",
    "#Get frequencies of trigrams\n",
    "engTriDist = nltk.FreqDist(engTrigrams)\n",
    "itTriDist = nltk.FreqDist(itTrigrams)\n",
    "\n",
    "#Probabilities\n",
    "\n",
    "engTrigramProb = []\n",
    "for word in eTest:\n",
    "    #Since we are doing trigrams, skip words which are less than 3 letters long\n",
    "    if len(word) < 3: continue\n",
    "    tempTrigrams = list(nltk.trigrams(word))\n",
    "    wordList = []\n",
    "    \n",
    "    wordList.append(word)\n",
    "    eProb = 1\n",
    "    iProb = 1\n",
    "    \n",
    "    for gram in tempTrigrams:\n",
    "        if gram in engTriDist.keys():\n",
    "            y = engTriDist[gram]\n",
    "            x = 0\n",
    "            for z in engTrigrams:\n",
    "                if z[0] == gram[0] and z[1] == gram[1]: x += 1\n",
    "            eProb *= y/x\n",
    "        if gram in itTriDist.keys():\n",
    "            y = itTriDist[gram]\n",
    "            x = 0\n",
    "            for z in itTrigrams:\n",
    "                if z[0] == gram[0] and z[1] == gram[1]: x += 1\n",
    "            iProb *= y/x\n",
    "    #If a probability remains 1, it did not occur\n",
    "    if eProb is 1: eProb = 0\n",
    "    if iProb is 1: iProb = 0\n",
    "    wordList.append(eProb)\n",
    "    wordList.append(iProb)\n",
    "    engTrigramProb.append(wordList)\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "#Get Accuracy\n",
    "eng_pred2 = []\n",
    "eng_true2 = []\n",
    "\n",
    "for i in engTrigramProb:\n",
    "    eng_true2.append(\"e\")\n",
    "    if i[1] > i[2]:\n",
    "        eng_pred2.append(\"e\")\n",
    "    elif i[2] > i[1]:\n",
    "        eng_pred2.append(\"i\")\n",
    "    else:\n",
    "        eng_pred2.append(\"ei\")\n",
    "        \n",
    "        \n",
    "engTrigramAccuracy = accuracy_score(eng_true2, eng_pred2)\n",
    "print(\"Accuracy:\\t\",engTrigramAccuracy * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
